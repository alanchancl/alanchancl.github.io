---
layout: post
title: 在基于图像转换的防御方法基础上引入了softmax分布分类器
date: 2021-09-09 15:20:17
categories: 论文笔记
tags: AI安全 对抗攻击
description: 在这篇文章中，我们探讨了一种新的防御方法，通过引入softmax分布分类器来增强基于图像转换的防御机制。该方法旨在提高对抗样本和干净样本的分类准确率，解决了传统随机输入变换方法在提升对抗样本准确率时对干净样本正确率的负面影响。

---

>- 论文名称：[《Enhancing Transformation-Based Defenses Against Adversarial Attacks With A Distribution Classifier》](http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1906.00258%3Fcontext%3Dcs.CV)
>- 作者单位：Connie Kou/ National University of Singapore
>- 收录时间：2020 ICLR
>- 文章亮点： 在之前Transformation-Based Defenses的基础上引入了softmax分布分类器，提高了干净和对抗样本的分类准确率。

<!--more-->

## Motivation

- 目前对抗样本的防御方法中，已经提出了随机输入变换方法。其思想是通过随机变换将图像从对抗攻击中恢复出来，并以多数表决作为随机样本之间的共识。

- 但是上述的随机输入变换方法在提高对抗样本的准确率的同时，牺牲了对干净样本的正确率。

- 本文研究了随机变换所导致的softmax分布，发现在干净样本上进行随机变化，softmax分布虽然会移动到错误的类别，但是却可以用softmax分布来修正预测。且对抗样本经过图像转化，其softmax分布与干净图像的分布形状类似

- 基于上述描述，本文提出了一种方法以提升现存的基于转换的防御：单独训练一个轻量级分布分类器来识别变换图像的softmax输出分布的不同特征。

- 实验证明，仅通过干净样本得到的分布进行分布分类器训练的防御效果，优于对干净样本和对抗样本的多数投票方法。

- 这种方法与攻击方法无关，不需要对CNN进行再训练，可以与现有的转换方法进行集成。

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" style="text-align: center;">
        {% include figure.liquid path="https://raw.githubusercontent.com/alanchancl/BlogImg/main/img202211231732657.png" title="Transformation-Based Defenses" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">Transformation-Based Defenses</div>


## Related Work

* 随机输入变化的防御方法：通过对输入图像的随机转换（缩小、旋转等操作），起到消解扰动，达到让模型正确分类的效果。
* Pixel deflection (PD) (Prakash et al., 2018)：随机打乱图像中的像素起到防御作用。
* Random resize and padding (RRP) (Xie et al., 2017) :随机调整大小，然后以随机方式用零填充到固定大小。
* transformation-based methods优点在于我们不需要对CNN分类模型再进行训练，但是存在一个问题：虽然对抗样本的分类正确率提升了，但是干净样本的分类正确率却下降了。
* 因此本文通过研究随机图像变换对softmax输出的分布影响，说明了变换后干净样本分类正确率的原因。

## 随机图像变换的softmax分布分析

* 利用MNIST数据集，观察干净样本和对抗样本在进行随机转换后的softmax分布。经过随机转换之后，我们发现一些干净图像会被错误地分类到求他类别（如label 8中的2、3），但是这些错误分类却与对抗样本随即转换后的投票类别相同。说明经过随机转换之后，干净样本和对抗样本具有相似的softmax分布。

* 利用JS divergence来定义softmax分布的距离。

* 并在不同转化强度(不同d值)的情况下，分别对clean-clean(same class)、adversarial-adversarial(same class)、clean-adversarial(same class)和clean-clean(different class)四种情况下进行softmax分布距离的计算。且值得注意的是，在d=300时，clean-clean(different class)蓝线减少的较快，且高于clean-clean(same class) 红线，这说明了转换依旧保留了不同类之间的差别。在d=800时，四条线都收敛了，这表明转化过大，不同类别之间的差异已经被不再保留。

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" style="text-align: center;">
        {% include figure.liquid path="https://raw.githubusercontent.com/alanchancl/BlogImg/main/img202211231732135.png" title="随机图像变换的softmax分布分析" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">随机图像变换的softmax分布分析</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" style="text-align: center;">
        {% include figure.liquid path="https://raw.githubusercontent.com/alanchancl/BlogImg/main/img202211231732693.jpg" title="softmax分布距离分析" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">softmax分布距离分析</div>

## Approach
利用softmax分类器改进防御方法：

- 训练阶段：只对干净的样本转换后得到的softmax分布与其对应的正确标签作为分布分类器的训练样本，训练模型可以根据softmax分布将干净样本正确分类。
- 测试阶段：对干净样本和对抗样本均作测试，以获得最终的类别预测。

## Experiments
利用softmax分类器改进防御方法：

- 数据集：MNIST、CIFAR10、CIFAR100
- 攻击方法：FGSM、IGSM、DeepFool、C&W
- Transformation-based defenses：PD、RRP
- 说明：因为实验旨在测试算法的有效性，因此只考虑CNN正确预测的图像。所以没有任何防御的CNN，分类干净样本时，正确率为100%，分类对抗样本时，正确率为0%。
- 结果：分类效果均较之前转换的防御方法有所提升。