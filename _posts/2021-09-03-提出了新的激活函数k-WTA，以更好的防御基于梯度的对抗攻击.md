---
layout: post
title: 提出了新的激活函数k-WTA，以更好的防御基于梯度的对抗攻击
date: 2021-09-03 12:20:17
categories: 论文笔记
tags: AI安全 对抗攻击
description: 提出了k-Winners-Take-All (k-WTA) 激活函数，这是一种C0不连续函数，旨在更有效地防御基于梯度的对抗攻击，从而提高模型的鲁棒性和分类准确率。
---

>- 论文名称：[《Enhancing Adversarial Defense By k-Winners-Take-All》](http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1905.10510v3)
>- 作者单位：Chang Xiao/ Columbia University
>- 收录时间：2020 ICLR
>- 文章亮点：提出了新的激活函数k-Winners-Take-All (k-WTA)，一种C0不连续函数，以更好的防御基于梯度的对抗攻击。

<!--more-->

## Motivation：

目前位置，生成对抗样本最有效的方法就是利用网络模型的梯度信息。但是目前面临着难以同时追求模型正确率和鲁棒性的两难境地，因此一系列利用模糊梯度作为防御机制的方法应运而生，例如：人为的离散化输入(Buckman et al., 2018; Lin et al., 2019)、对输入引入一定随机噪声(Xie et al., 2018a; Guo et al., 2018)、对网络结构引入一定随机噪声(Dhillon et al., 2018; Cohen et al., 2019)。但是针对以上模糊化梯度的防御方法：Athalye et al. (2018)指出，模糊化的梯度任可以被近似，因此以上方法任然脆弱。

### 文章的技术贡献
1. 不模糊化梯度、而是让梯度未定义：
    只需要简单的通过将标准神经网络中的激活函数（如ReLU），改变为C0不连续函数，即k-Winners-Take-All (k-WTA)。神经网络中的其余组成和训练方法均不需要改变。

2. 解释为什么更换k-WTA激活函数有利于提高对抗的鲁棒性：
    如下图为利用k-WTA模型生成的间断、密集、不连续的带有红点的短蓝直线去拟合一维函数（绿色虚线）的示意图。我们可以知道，由于在不连续性中，梯度也是无法确认的，因此攻击者对对抗样本的搜索的将会变得盲目。同时这样子也不影响网络的正常分类训练。因此具有可行性。

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" style="text-align: center;">
        {% include figure.liquid path="https://raw.githubusercontent.com/alanchancl/BlogImg/main/img202211231628957.png" title="拟合一维函数的示意图" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">拟合一维函数的示意图</div>

1. 实验验证：
对不同网络架构下的多个数据集进行了广泛的实验，包括：ResNet、DenseNet和Wide ResNet。并在多种白盒攻击下，将建议的k-WTA激活和常用的ReLU激活的鲁棒性性能做了对比，实验证明：仅通过改变k-WTA激活就可以提高鲁棒性，防御基于梯度的对抗攻击。

## Related Work：

通过隐藏梯度信息从而实现防御的方法称为梯度模糊或者梯度遮掩。

这种方法主要分为两类：一类是利用随机性：通过输入随机或者网络中引入随机层，但是这种方法可以通过多次实验去平均值来估算从而破解。第二类是通过故意离散输入使得网络梯度不存在或者不正确，或者人为的提高数值不稳定性进行梯度评估。但是这种方法也容易收到攻击。

k-WTA激活不仅模糊了网络的梯度，而且在某些输入样本出将其破坏了。即使有利用平滑近似的攻击可以估算某一层的梯度信息，但是该信息值的误差将会在每一层进行累计，最后以达到防御任何基于梯度估计的攻击（例如BPDA）。

## k-Winners-Take-All Activation
* k-Winners-Take-All Activation (k-WTA激活)来源于KWTA

* LWTA和k-WTA的差别：

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" style="text-align: center;">
        {% include figure.liquid path="https://raw.githubusercontent.com/alanchancl/BlogImg/main/img202211231630311.png" title="LWTA和k-WTA的差别" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">LWTA和k-WTA的差别</div>

## Experiment

* 无论使用什么训练方法，k-WTA都能普遍提高白盒鲁棒性。
* k-WTA在黑盒攻击下的鲁棒性并不总是明显优于ReLU网络。
* 稀疏度γ也对网络的鲁棒性有影响。